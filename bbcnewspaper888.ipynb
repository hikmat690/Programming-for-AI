{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "bbcnewspaper888.ipynb",
      "authorship_tag": "ABX9TyNWW0SW3r3nTFqDy/J9aBfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmat690/AI-programming/blob/main/bbcnewspaper888.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis (Text Classification)**\n",
        "*   **Downloading Datset from Kaggle to Google Colab**\n",
        "*   **Text Cleaning**\n",
        "*   **BERT Model (Feature Engineering)**\n",
        "*   **DL Model**"
      ],
      "metadata": {
        "id": "QKxILf5ndoUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Preprocessing Libraries**"
      ],
      "metadata": {
        "id": "Opp1GMraebjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"tensorflow-text==2.13.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGrfHJPzwA8y",
        "outputId": "bd17ac7a-9464-46d1-8bfb-ab2d4672b72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text==2.13.* in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.13.*) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.12.1)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.37.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.13.*) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --quiet tensorflow_text\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score,precision_score,accuracy_score,confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation"
      ],
      "metadata": {
        "id": "S5-bcmcNee4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe70358-2243-4c6b-9a2c-ee6dd8db7b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data**"
      ],
      "metadata": {
        "id": "A4QQYCBbeonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.read_csv('/content/bbc-text.csv')\n",
        "df=temp.iloc[:1200]"
      ],
      "metadata": {
        "id": "IT9OMIfMe0jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "oBxAIuO7VL3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb85f56c-72c7-428c-96b7-fb36043ef54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stopwords])\n",
        "\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "    Remove special characters from the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input string.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned string with only alphanumeric characters and spaces.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^A-Za-z0-9\\s]', '', text)\n"
      ],
      "metadata": {
        "id": "M3lxel6rwsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning & Preprocessing**"
      ],
      "metadata": {
        "id": "KkZ1tUgelQgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "df['text'] = df['text'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "6dWUTvfVUrGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bded8e-f2ff-493f-8396-f137a4cee2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-28cab5627bd4>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].str.lower()\n",
            "<ipython-input-41-28cab5627bd4>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].apply(remove_stopwords)\n",
            "<ipython-input-41-28cab5627bd4>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].apply(remove_special_characters)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']"
      ],
      "metadata": {
        "id": "aXe4x2noU03H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "9c200160-2e7f-42e5-c554-1f5d56d36993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tv future hands viewers home theatre systems p...\n",
              "1       worldcom boss left books alone former worldcom...\n",
              "2       tigers wary farrell gamble leicester say rushe...\n",
              "3       yeading face newcastle fa cup premiership side...\n",
              "4       ocean twelve raids box office ocean twelve cri...\n",
              "                              ...                        \n",
              "1195    duff ruled barcelona clash chelsea damien duff...\n",
              "1196    original exorcist screened original version ho...\n",
              "1197    record year chilean copper chile copper indust...\n",
              "1198    sales fail boost high street january sales fai...\n",
              "1199    church anger bollywood film roman catholic org...\n",
              "Name: text, Length: 1200, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tv future hands viewers home theatre systems p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>worldcom boss left books alone former worldcom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tigers wary farrell gamble leicester say rushe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yeading face newcastle fa cup premiership side...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ocean twelve raids box office ocean twelve cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>duff ruled barcelona clash chelsea damien duff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>original exorcist screened original version ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>record year chilean copper chile copper indust...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>sales fail boost high street january sales fai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>church anger bollywood film roman catholic org...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1xFFxHFOq0C_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "38eb5aa6-a2fc-41c5-cf60-9de0f5e12b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category    0\n",
              "text        0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "zV6bPViYl3_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Column Encoding**"
      ],
      "metadata": {
        "id": "JJozhru2MDY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df['text']\n",
        "Y = df['category']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(Y)\n",
        "\n",
        "print(Y)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(df['text'],Y,test_size=0.2,random_state=42)\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "7ZflBvOgstHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b517bc4-6621-4708-d151-42ea2b99fe42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 0 3 ... 0 0 1]\n",
            "331     cebit fever takes hanover thousands products t...\n",
            "409     eu aiming fuel development aid european union ...\n",
            "76      yukos sues four firms 20bn russian oil firm yu...\n",
            "868     moody joins england lewis moody flown dublin j...\n",
            "138     safin relieved aussie recovery marat safin adm...\n",
            "                              ...                        \n",
            "1044    domain system scam fear system make easier cre...\n",
            "1095    african double edinburgh world 5000m champion ...\n",
            "1130    price trusted pc security buy trusted computer...\n",
            "860     new year texting breaks record mobile phone es...\n",
            "1126    stuart joins norwich addicks norwich signed ch...\n",
            "Name: text, Length: 960, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finetuning using Deep Learning**"
      ],
      "metadata": {
        "id": "aQMkc_XiUiFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the BERT tokenizer and model (bert-base-uncased)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the maximum sequence length to 128\n",
        "max_length = 128\n",
        "\n",
        "# Function to tokenize and preprocess text data\n",
        "def preprocess_text(text_data):\n",
        "    # Tokenize the text data using the BERT tokenizer\n",
        "    encoding = tokenizer(\n",
        "        text_data,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf',  # Return as TensorFlow tensors\n",
        "        add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
        "    )\n",
        "    return encoding\n",
        "\n",
        "# Function to extract embeddings from the fine-tuned BERT model\n",
        "def extract_embeddings(text_data):\n",
        "    # Preprocess the input text data\n",
        "    # Convert the input to a list of strings if it's a Pandas Series\n",
        "    if isinstance(text_data, pd.Series):\n",
        "        text_data = text_data.tolist()\n",
        "    encoding = preprocess_text(text_data)\n",
        "\n",
        "    # Extract embeddings from BERT (use the output of the BERT model)\n",
        "    outputs = bert_model(encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
        "\n",
        "    # We are interested in the 'pooler_output' (the representation of [CLS] token)\n",
        "    embeddings = outputs.pooler_output  # (batch_size, hidden_size)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Example dataset (replace this with your actual data)\n",
        "X = df['text']\n",
        "Y = df['category']\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(Y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get the embeddings for both training and testing sets\n",
        "X_train_embeddings = extract_embeddings(X_train)\n",
        "X_test_embeddings = extract_embeddings(X_test)\n",
        "\n",
        "# Convert embeddings to numpy arrays (if needed)\n",
        "X_train_embeddings = np.array(X_train_embeddings)\n",
        "X_test_embeddings = np.array(X_test_embeddings)\n",
        "\n",
        "# Print the shape of embeddings\n",
        "print(X_train_embeddings.shape)  # Should print (batch_size, 768)\n",
        "print(X_test_embeddings.shape)   # Should print (batch_size, 768)\n",
        "\n",
        "# ... (rest of the code) ...  # Should print (batch_size, 768)\n",
        "\n",
        "# 1. Define an Input layer with the shape of your embeddings\n",
        "input_layer = tf.keras.Input(shape=(X_train_embeddings.shape[1],), name='input_embeddings')\n",
        "\n",
        "# 2. Apply Dropout and Dense layers to the input layer\n",
        "drop_out = tf.keras.layers.Dropout(0.2, name='dropout')(input_layer)\n",
        "output = tf.keras.layers.Dense(5, activation='softmax', name='output')(drop_out)\n",
        "\n",
        "# 3. Create the Keras model using the input and output layers\n",
        "model = tf.keras.Model(inputs=[input_layer], outputs=[output])\n",
        "\n",
        "# Compile the model with RMSprop optimizer\n",
        "optimizer = RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert y_train and y_test to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=5)\n",
        "y_test = to_categorical(y_test, num_classes=5)\n",
        "\n",
        "# Train the model with mini-batch size of 8 and 4 epochsalidation_data=(X_test_embeddings, y_test))\n",
        "history = model.fit(X_train_embeddings, y_train, epochs=4, batch_size=8, validation_data=(X_test_embeddings, y_test))\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "xWZDBNbMYWvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13699332-2f3e-4bf8-d8b9-7b00d4e7dbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(960, 768)\n",
            "(240, 768)\n",
            "Epoch 1/4\n",
            "120/120 [==============================] - 1s 3ms/step - loss: 16.9084 - accuracy: 0.4031 - val_loss: 13.7415 - val_accuracy: 0.3917\n",
            "Epoch 2/4\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 7.9024 - accuracy: 0.6375 - val_loss: 9.1268 - val_accuracy: 0.6625\n",
            "Epoch 3/4\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 6.8322 - accuracy: 0.6896 - val_loss: 3.6404 - val_accuracy: 0.8292\n",
            "Epoch 4/4\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 6.4504 - accuracy: 0.7260 - val_loss: 7.2195 - val_accuracy: 0.6833\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_embeddings (InputLay  [(None, 768)]             0         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 768)               0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 5)                 3845      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3845 (15.02 KB)\n",
            "Trainable params: 3845 (15.02 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get the embeddings for both training and testing sets\n",
        "X_train_embeddings = extract_embeddings(X_train)\n",
        "X_test_embeddings = extract_embeddings(X_test)\n"
      ],
      "metadata": {
        "id": "GhOHfPgg-AMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest model on the BERT embeddings\n",
        "rf.fit(X_train_embeddings, y_train.argmax(axis=1))  # y_train is one-hot encoded, use argmax to get class labels\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test_embeddings)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "# Convert y_test to class labels using argmax to match y_pred format\n",
        "accuracy = accuracy_score(y_test.argmax(axis=1), y_pred)\n",
        "print(f\"Random Forest model accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRkO9U74-HOd",
        "outputId": "9c024d19-85ba-4cf0-98b9-e43ef1524e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest model accuracy: 0.8167\n"
          ]
        }
      ]
    }
  ]
}
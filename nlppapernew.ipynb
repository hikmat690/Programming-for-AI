{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCZTOtWaaq4tVDuLTEXt4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmat690/Programming-for-AI/blob/main/nlppapernew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b6nZtsLlVGo",
        "outputId": "26259cc6-fd36-4d85-a850-0398216d3a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CPO'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 41 (delta 15), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 12.80 MiB | 4.08 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "/content/CPO\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827724 sha256=d0fddc53b35dd04bb95b84bbfd0aa40e3dedac442357a2448426835be7e1ac0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
            "Successfully built gym\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gym, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n"
          ]
        }
      ],
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/201736621051/CPO.git\n",
        "%cd CPO\n",
        "\n",
        "# Install dependencies (compatible with current Colab)\n",
        "!pip install torch==2.2.2 gym==0.26.2 numpy matplotlib scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2 transformers==4.39.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RPcDwBPam6Im",
        "outputId": "325e4fed-3f31-4eea-d8e6-55dc1c09dbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting transformers==4.39.3\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.39.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "422afad7e68f45b88cba08629b598f13"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, OPTForCausalLM, OPTConfig, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
        "from transformers import TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, AutoModel, PretrainedConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler, PreTrainedModel\n",
        "from transformers.modeling_utils import *\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n",
        "from transformers.models.llama.modeling_llama import *\n",
        "from transformers.models.opt.modeling_opt import *\n",
        "from transformers.models.gpt_neox.modeling_gpt_neox import *\n",
        "from transformers.tokenization_utils_base import *\n",
        "from transformers.data.data_collator import *\n",
        "from transformers.utils import *\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from torch.utils import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import *\n",
        "import argparse\n",
        "import csv\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, Accelerator\n",
        "import deepspeed\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "import copy as cp\n",
        "import numpy as np\n",
        "from deepspeed.runtime.utils import see_memory_usage\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import math\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers.utils import is_torch_tpu_available\n",
        "from baukit import Trace, TraceDict\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model_name', type=str,help='模型名称')\n",
        "parser.add_argument('--dataset_name', type=str,help='数据集名称')\n",
        "parser.add_argument('--mode', type=str,help='训练策略')\n",
        "parser.add_argument('--cur_epoch', type=int,help='训练策略')\n",
        "parser.add_argument('--worst_k', type=int,help='训练策略')\n",
        "parser.add_argument('--domain_name', type=str,help='数据集域名')\n",
        "parser.add_argument('--local_rank', type=int, default=-1)\n",
        "parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of epochs to train for.\")\n",
        "parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size to use for training.\")\n",
        "parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size to use for testing.\")\n",
        "parser.add_argument(\"--generation_max_length\", type=int, default=2000, help=\"Maximum length to use for generation\")\n",
        "parser.add_argument(\"--generation_num_beams\", type=int, default=4, help=\"Number of beams to use for generation.\")\n",
        "parser.add_argument(\"--lr\", type=float, default=3e-3, help=\"Learning rate to use for training.\")\n",
        "parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed to use for training.\")\n",
        "parser.add_argument(\"--deepspeed\", type=str, default=None, help=\"Path to deepspeed config file.\")\n",
        "parser.add_argument(\n",
        "    \"--bf16\",\n",
        "    type=bool,\n",
        "    default=True if torch.cuda.get_device_capability()[0] == 8 else False,\n",
        "    help=\"Whether to use bf16.\",\n",
        ")\n",
        "args = parser.parse_args()\n",
        "print(args.dataset_name)\n",
        "print(args.model_name)\n",
        "label_names = None\n",
        "if args.mode == 't_y_f_y':\n",
        "    label_names = [\"labels\"]\n",
        "if args.mode == 't_y_f_n':\n",
        "    label_names = [\"labels\",\"true_false_labels\"]\n",
        "if args.mode == 'con':\n",
        "    label_names = [\"labels\",\"true_false_labels\"]\n",
        "if args.mode == 'procon' or args.mode == 'proun':\n",
        "    label_names = [\"labels\",\"true_false_labels\"]\n",
        "\n",
        "\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"<s>\"\n",
        "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
        "IGNORE_INDEX = -100\n",
        "index_list = [1, 0, 30, 31]\n",
        "index_list = index_list[:args.worst_k]\n",
        "modules_list = []\n",
        "query_key_value = []\n",
        "dense = []\n",
        "h_to_4h = []\n",
        "h4_to_h = []\n",
        "for i in index_list:\n",
        "    query_key_value.append(f\"gpt_neox.layers.{i}.attention.query_key_value.weight\")\n",
        "    dense.append(f\"gpt_neox.layers.{i}.attention.dense.weight\")\n",
        "    h_to_4h.append(f\"gpt_neox.layers.{i}.mlp.dense_h_to_4h.weight\")\n",
        "    h4_to_h.append(f\"gpt_neox.layers.{i}.mlp.dense_4h_to_h.weight\")\n",
        "modules_list = query_key_value + dense + h_to_4h + h4_to_h\n",
        "LAYER_NUM = 32\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"../model/\"+args.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "def batch_preprocessing_t_y_f_y(batch):\n",
        "    batch_input_ids = []\n",
        "    batch_attention_mask = []\n",
        "    batch_labels = []\n",
        "    if args.dataset_name == 'XSUM':\n",
        "        for article, summary, sen_label in zip(batch['article'],batch['summary'],batch['label']):\n",
        "            if len(sen_label) == 0:\n",
        "                prompt = \"Article: \" + article + \"\\nWrite a summary consistent with the above article in no more than 40 words:\\n\"\n",
        "            else:\n",
        "                prompt = \"Article: \" + article + \"\\nWrite a summary inconsistent with the above article in no more than 40 words:\\n\"\n",
        "                #(1,seq_len)\n",
        "            prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "            prompt_tokenized['labels'] = torch.full_like(prompt_tokenized['input_ids'], IGNORE_INDEX)\n",
        "            if len(sen_label) == 0:#真实\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    #(1,seq_len)\n",
        "                    prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                    prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "            else:#幻觉\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    if summary_sen_index in sen_label:\n",
        "                        prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    else:\n",
        "                        prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], torch.full_like(summary_sen_ids['input_ids'], IGNORE_INDEX)),dim=1)\n",
        "            eos_token_tokenized = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", add_special_tokens=False)\n",
        "            prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], eos_token_tokenized['attention_mask']),dim=1)\n",
        "            prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            batch_input_ids.append(prompt_tokenized['input_ids'].squeeze())\n",
        "            batch_attention_mask.append(prompt_tokenized['attention_mask'].squeeze())\n",
        "            batch_labels.append(prompt_tokenized['labels'].squeeze())\n",
        "        example = {}\n",
        "        example['input_ids'] = pad_sequence(batch_input_ids,batch_first=True,padding_value=tokenizer.pad_token_id)\n",
        "        example['attention_mask'] = pad_sequence(batch_attention_mask,batch_first=True,padding_value=0)\n",
        "        example['labels'] = pad_sequence(batch_labels,batch_first=True,padding_value=-100)\n",
        "        if example['input_ids'].shape[1] > 2000:\n",
        "            example['input_ids'] = example['input_ids'][:,:2000]\n",
        "            example['attention_mask'] = example['attention_mask'][:,:2000]\n",
        "            example['labels'] = example['labels'][:,:2000]\n",
        "        return example\n",
        "    if args.dataset_name == 'CNNDM':\n",
        "        return\n",
        "    if args.dataset_name == 'TWEET':\n",
        "        return\n",
        "    if args.dataset_name == 'MEDIA':\n",
        "        return\n",
        "def batch_preprocessing_t_y_f_n(batch):\n",
        "    batch_input_ids = []\n",
        "    batch_attention_mask = []\n",
        "    batch_labels = []\n",
        "    batch_tf_labels = []\n",
        "    if args.dataset_name == 'XSUM':\n",
        "        for article, summary, sen_label in zip(batch['article'],batch['summary'],batch['label']):\n",
        "            prompt = \"Article: \" + article + \"\\nWrite a summary consistent with the above article in no more than 40 words:\\n\"\n",
        "            prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
        "            prompt_tokenized['labels'] = torch.full_like(prompt_tokenized['input_ids'], IGNORE_INDEX)\n",
        "            if len(sen_label) == 0:\n",
        "                batch_tf_labels.append(1)\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    #(1,seq_len)\n",
        "                    prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                    prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "            else:\n",
        "                batch_tf_labels.append(0)\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    if summary_sen_index in sen_label:\n",
        "                        prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    else:\n",
        "                        prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], torch.full_like(summary_sen_ids['input_ids'], IGNORE_INDEX)),dim=1)\n",
        "            eos_token_tokenized = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", add_special_tokens=False)\n",
        "            prompt_tokenized['input_ids'] = torch.cat((prompt_tokenized['input_ids'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            prompt_tokenized['attention_mask'] = torch.cat((prompt_tokenized['attention_mask'], eos_token_tokenized['attention_mask']),dim=1)\n",
        "            prompt_tokenized['labels'] = torch.cat((prompt_tokenized['labels'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            batch_input_ids.append(prompt_tokenized['input_ids'].squeeze())\n",
        "            batch_attention_mask.append(prompt_tokenized['attention_mask'].squeeze())\n",
        "            batch_labels.append(prompt_tokenized['labels'].squeeze())\n",
        "        example = {}\n",
        "        example['input_ids'] = pad_sequence(batch_input_ids,batch_first=True,padding_value=tokenizer.pad_token_id)\n",
        "        example['attention_mask'] = pad_sequence(batch_attention_mask,batch_first=True,padding_value=0)\n",
        "        example['labels'] = pad_sequence(batch_labels,batch_first=True,padding_value=-100)\n",
        "        if example['input_ids'].shape[1] > 2000:\n",
        "            example['input_ids'] = example['input_ids'][:,:2000]\n",
        "            example['attention_mask'] = example['attention_mask'][:,:2000]\n",
        "            example['labels'] = example['labels'][:,:2000]\n",
        "        example['true_false_labels'] = torch.tensor(batch_tf_labels)\n",
        "        return example\n",
        "    if args.dataset_name == 'CNNDM':\n",
        "        return\n",
        "    if args.dataset_name == 'TWEET':\n",
        "        return\n",
        "    if args.dataset_name == 'MEDIA':\n",
        "        return\n",
        "\n",
        "def batch_preprocessing_con(batch):\n",
        "    batch_input_ids = []\n",
        "    batch_attention_mask = []\n",
        "    batch_labels = []\n",
        "    batch_con_labels = []\n",
        "    if args.dataset_name == 'XSUM':\n",
        "        for article, summary, sen_label in zip(batch['article'],batch['summary'],batch['label']):\n",
        "            if len(sen_label) == 0:\n",
        "                consist_prompt = \"Article: \" + article + \"\\nWrite a summary consistent with the above article in no more than 40 words:\\n\"\n",
        "                conflict_prompt = \"Article: \" + article + \"\\nWrite a summary inconsistent with the above article in no more than 40 words:\\n\"\n",
        "            else:\n",
        "                consist_prompt = \"Article: \" + article + \"\\nWrite a summary inconsistent with the above article in no more than 40 words:\\n\"\n",
        "                conflict_prompt = \"Article: \" + article + \"\\nWrite a summary consistent with the above article in no more than 40 words:\\n\"\n",
        "            #(1,seq_len)\n",
        "            consist_prompt_tokenized = tokenizer(consist_prompt, return_tensors=\"pt\")\n",
        "            consist_prompt_tokenized['labels'] = torch.full_like(consist_prompt_tokenized['input_ids'], IGNORE_INDEX)\n",
        "            conflict_prompt_tokenized = tokenizer(conflict_prompt, return_tensors=\"pt\")\n",
        "            conflict_prompt_tokenized['labels'] = torch.full_like(conflict_prompt_tokenized['input_ids'], IGNORE_INDEX)\n",
        "\n",
        "            if len(sen_label) == 0:#真实\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    #(1,seq_len)\n",
        "                    consist_prompt_tokenized['input_ids'] = torch.cat((consist_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    consist_prompt_tokenized['attention_mask'] = torch.cat((consist_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                    consist_prompt_tokenized['labels'] = torch.cat((consist_prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    #conflict指令中，所有无幻觉的句子都要被惩罚\n",
        "                    conflict_prompt_tokenized['input_ids'] = torch.cat((conflict_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    conflict_prompt_tokenized['attention_mask'] = torch.cat((conflict_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                    conflict_prompt_tokenized['labels'] = torch.cat((conflict_prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "            else:#幻觉\n",
        "                for summary_sen_index in range(len(summary)):\n",
        "                    summary_sen_ids = tokenizer(summary[summary_sen_index], return_tensors=\"pt\", add_special_tokens=False)\n",
        "                    if summary_sen_index in sen_label:\n",
        "                        #consist指令中，有幻觉的句子直接拼，符合指令，cross entropy鼓励\n",
        "                        consist_prompt_tokenized['input_ids'] = torch.cat((consist_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        consist_prompt_tokenized['attention_mask'] = torch.cat((consist_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        consist_prompt_tokenized['labels'] = torch.cat((consist_prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        #conflict指令中，有幻觉的句子直接拼，有冲突，都要被惩罚\n",
        "                        conflict_prompt_tokenized['input_ids'] = torch.cat((conflict_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        conflict_prompt_tokenized['attention_mask'] = torch.cat((conflict_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        conflict_prompt_tokenized['labels'] = torch.cat((conflict_prompt_tokenized['labels'], summary_sen_ids['input_ids']),dim=1)\n",
        "                    else:\n",
        "                        #consist中，无幻觉的句子mask，没有符合指令，不用鼓励\n",
        "                        consist_prompt_tokenized['input_ids'] = torch.cat((consist_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        consist_prompt_tokenized['attention_mask'] = torch.cat((consist_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        consist_prompt_tokenized['labels'] = torch.cat((consist_prompt_tokenized['labels'], torch.full_like(summary_sen_ids['input_ids'], IGNORE_INDEX)),dim=1)\n",
        "                        #conflict中，无幻觉的句子mask，没有冲突，不用惩罚\n",
        "                        conflict_prompt_tokenized['input_ids'] = torch.cat((conflict_prompt_tokenized['input_ids'], summary_sen_ids['input_ids']),dim=1)\n",
        "                        conflict_prompt_tokenized['attention_mask'] = torch.cat((conflict_prompt_tokenized['attention_mask'], summary_sen_ids['attention_mask']),dim=1)\n",
        "                        conflict_prompt_tokenized['labels'] = torch.cat((conflict_prompt_tokenized['labels'], torch.full_like(summary_sen_ids['input_ids'], IGNORE_INDEX)),dim=1)\n",
        "            eos_token_tokenized = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", add_special_tokens=False)\n",
        "            consist_prompt_tokenized['input_ids'] = torch.cat((consist_prompt_tokenized['input_ids'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            consist_prompt_tokenized['attention_mask'] = torch.cat((consist_prompt_tokenized['attention_mask'], eos_token_tokenized['attention_mask']),dim=1)\n",
        "            consist_prompt_tokenized['labels'] = torch.cat((consist_prompt_tokenized['labels'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            conflict_prompt_tokenized['input_ids'] = torch.cat((conflict_prompt_tokenized['input_ids'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            conflict_prompt_tokenized['attention_mask'] = torch.cat((conflict_prompt_tokenized['attention_mask'], eos_token_tokenized['attention_mask']),dim=1)\n",
        "            conflict_prompt_tokenized['labels'] = torch.cat((conflict_prompt_tokenized['labels'], eos_token_tokenized['input_ids']),dim=1)\n",
        "            batch_input_ids.append(consist_prompt_tokenized['input_ids'].squeeze())\n",
        "            batch_attention_mask.append(consist_prompt_tokenized['attention_mask'].squeeze())\n",
        "            batch_labels.append(consist_prompt_tokenized['labels'].squeeze())\n",
        "            batch_con_labels.append(1)\n",
        "            batch_input_ids.append(conflict_prompt_tokenized['input_ids'].squeeze())\n",
        "            batch_attention_mask.append(conflict_prompt_tokenized['attention_mask'].squeeze())\n",
        "            batch_labels.append(conflict_prompt_tokenized['labels'].squeeze())\n",
        "            batch_con_labels.append(0)\n",
        "        example = {}\n",
        "        example['input_ids'] = pad_sequence(batch_input_ids,batch_first=True,padding_value=tokenizer.pad_token_id)\n",
        "        example['attention_mask'] = pad_sequence(batch_attention_mask,batch_first=True,padding_value=0)\n",
        "        example['labels'] = pad_sequence(batch_labels,batch_first=True,padding_value=-100)\n",
        "        if example['input_ids'].shape[1] > 2000:\n",
        "            example['input_ids'] = example['input_ids'][:,:2000]\n",
        "            example['attention_mask'] = example['attention_mask'][:,:2000]\n",
        "            example['labels'] = example['labels'][:,:2000]\n",
        "        example['true_false_labels'] = torch.tensor(batch_con_labels)\n",
        "        return example\n",
        "    if args.dataset_name == 'CNNDM':\n",
        "        return\n",
        "    if args.dataset_name == 'TWEET':\n",
        "        return\n",
        "    if args.dataset_name == 'MEDIA':\n",
        "        return\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):\n",
        "        global modules_list\n",
        "        if self.control.should_log:\n",
        "            if is_torch_tpu_available():\n",
        "                xm.mark_step()\n",
        "\n",
        "            logs: Dict[str, float] = {}\n",
        "\n",
        "            # all_gather + mean() to get average loss over all processes\n",
        "            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()\n",
        "\n",
        "            # reset tr_loss to zero\n",
        "            tr_loss -= tr_loss\n",
        "\n",
        "            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n",
        "            logs[\"learning_rate\"] = self._get_learning_rate()\n",
        "\n",
        "            self._total_loss_scalar += tr_loss_scalar\n",
        "            self._globalstep_last_logged = self.state.global_step\n",
        "            self.store_flos()\n",
        "\n",
        "            self.log(logs)\n",
        "\n",
        "        metrics = None\n",
        "\n",
        "        if self.control.should_evaluate:\n",
        "            if isinstance(self.eval_dataset, dict):\n",
        "                metrics = {}\n",
        "                for eval_dataset_name, eval_dataset in self.eval_dataset.items():\n",
        "                    if eval_dataset_name == \"test\":\n",
        "                        dataset_metrics = self.evaluate(\n",
        "                            eval_dataset=eval_dataset\n",
        "                        )\n",
        "                        metrics.update(dataset_metrics)\n",
        "                    else:\n",
        "                        eval_dataloader = DataLoader(eval_dataset, batch_size=1, collate_fn=default_data_collator)\n",
        "                        all_hidden_states = []\n",
        "                        all_labels = []\n",
        "                        model.eval()\n",
        "                        step = 1\n",
        "                        i = 1\n",
        "                        for batch in tqdm(eval_dataloader):\n",
        "                            with torch.no_grad():\n",
        "                                input_ids = batch['input_ids']\n",
        "                                att = batch['attention_mask']\n",
        "                                input_ids = input_ids.to(model.device)\n",
        "                                att = att.to(model.device)\n",
        "                                output = model(input_ids = input_ids, attention_mask = att, output_hidden_states = True)\n",
        "                                hidden_states = output.hidden_states\n",
        "                                hidden_states = torch.stack(hidden_states, dim = 0)\n",
        "                                hidden_states = hidden_states.detach().cpu()\n",
        "                                #layer batch seq dim\n",
        "                                all_hidden_states.append(hidden_states[:,:,-1,:].squeeze(2).transpose(0,1))\n",
        "                                all_labels.append(batch['true_false_labels'])\n",
        "                                step = step + 1\n",
        "                                if step == 200:\n",
        "                                    np.save('probing/pythia-'+args.mode+'/all_hidden_states' + str(i) + '.npy', torch.cat(all_hidden_states, dim = 0))\n",
        "                                    np.save('probing/pythia-'+args.mode+'/all_labels' + str(i) + '.npy', torch.cat(all_labels, dim = 0))\n",
        "                                    all_hidden_states = []\n",
        "                                    all_labels = []\n",
        "                                    i = i + 1\n",
        "                                    step = 1\n",
        "                        np.save('probing/pythia-'+args.mode+'/all_hidden_states' + str(i) + '.npy', torch.cat(all_hidden_states, dim = 0))\n",
        "                        np.save('probing/pythia-'+args.mode+'/all_labels' + str(i) + '.npy', torch.cat(all_labels, dim = 0))\n",
        "                        all_hidden_states = []\n",
        "                        all_labels = []\n",
        "                        acc = np.zeros((LAYER_NUM,1))\n",
        "                        data = np.concatenate([np.load('probing/pythia-'+args.mode+'/all_hidden_states' + str(i+1) + '.npy') for i in range(19)], axis = 0)\n",
        "                        label = np.concatenate([np.load('probing/pythia-'+args.mode+'/all_labels' + str(i+1) + '.npy') for i in range(19)], axis = 0)\n",
        "                        all_X_train, all_X_val, y_train, y_val = train_test_split(data,label,test_size=0.2,random_state=42)\n",
        "                        for layer_id in range(LAYER_NUM):\n",
        "                            X_train = all_X_train[:,layer_id+1,:]\n",
        "                            X_val = all_X_val[:,layer_id+1,:]\n",
        "                            clf = LogisticRegression(random_state=42, max_iter=10000).fit(X_train, y_train)\n",
        "                            y_val_pred = clf.predict(X_val)\n",
        "                            acc[layer_id][0] = accuracy_score(y_val, y_val_pred)\n",
        "                        index_list = []\n",
        "                        for index in range(args.worst_k):\n",
        "                            index_list.append(np.argmin(acc))\n",
        "                            acc[np.argmin(acc)][0] = 1.\n",
        "                        modules_list = []\n",
        "                        query_key_value = []\n",
        "                        dense = []\n",
        "                        h_to_4h = []\n",
        "                        h4_to_h = []\n",
        "                        for i in index_list:\n",
        "                            query_key_value.append(f\"gpt_neox.layers.{i}.attention.query_key_value.weight\")\n",
        "                            dense.append(f\"gpt_neox.layers.{i}.attention.dense.weight\")\n",
        "                            h_to_4h.append(f\"gpt_neox.layers.{i}.mlp.dense_h_to_4h.weight\")\n",
        "                            h4_to_h.append(f\"gpt_neox.layers.{i}.mlp.dense_4h_to_h.weight\")\n",
        "                        modules_list = query_key_value + dense + h_to_4h + h4_to_h\n",
        "                        with open('/mnt/nas-coai-wlcb/fhwexp/prefix-tuning-13b/pythia-modules.json', 'w+') as fs:\n",
        "                            fs.write(json.dumps({\"modules\": modules_list})+'\\n')\n",
        "                            fs.close()\n",
        "            else:\n",
        "                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
        "            self._report_to_hp_search(trial, self.state.global_step, metrics)\n",
        "\n",
        "            # Run delayed LR scheduler now that metrics are populated\n",
        "            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                metric_to_check = self.args.metric_for_best_model\n",
        "                if not metric_to_check.startswith(\"eval_\"):\n",
        "                    metric_to_check = f\"eval_{metric_to_check}\"\n",
        "                self.lr_scheduler.step(metrics[metric_to_check])\n",
        "\n",
        "        if self.control.should_save:\n",
        "            self._save_checkpoint(model, trial, metrics=metrics)\n",
        "            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n",
        "\n",
        "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = LlamaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        true_false_labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
        "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            logits = torch.cat(logits, dim=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(hidden_states)\n",
        "        logits = logits.float()\n",
        "\n",
        "        loss = torch.tensor(0.).to(logits.device)\n",
        "\n",
        "        if labels is not None and true_false_labels is not None:\n",
        "            loss_fct_1 = CrossEntropyLoss()\n",
        "            loss_fct_2 = NLLLoss()\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Enable model parallelism\n",
        "            true_indices = torch.where(true_false_labels == 1)[0]\n",
        "            false_indices = torch.where(true_false_labels == 0)[0]\n",
        "            if true_indices.shape[0] != 0:\n",
        "                true_logits = torch.index_select(shift_logits, dim=0, index=true_indices)\n",
        "                true_labels = torch.index_select(shift_labels, dim=0, index=true_indices)\n",
        "                true_logits = true_logits.view(-1, self.config.vocab_size)\n",
        "                true_labels = true_labels.view(-1)\n",
        "                true_labels = true_labels.to(true_logits.device)\n",
        "                loss = loss + loss_fct_1(true_logits, true_labels)\n",
        "            if false_indices.shape[0] != 0:\n",
        "                false_logits = torch.index_select(shift_logits, dim=0, index=false_indices)\n",
        "                false_labels = torch.index_select(shift_labels, dim=0, index=false_indices)\n",
        "                false_logits = false_logits.view(-1, self.config.vocab_size)\n",
        "                false_labels = false_labels.view(-1)\n",
        "                false_labels = false_labels.to(false_logits.device)\n",
        "                loss = loss - 0.05 * loss_fct_2(torch.softmax(false_logits,dim=-1), false_labels)\n",
        "            # Flatten the tokens\n",
        "        # t_y_f_y\n",
        "        if labels is not None and true_false_labels is None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return (loss,) + output if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        position_ids = kwargs.get(\"position_ids\", None)\n",
        "        if attention_mask is not None and position_ids is None:\n",
        "            # create position_ids on the fly for batch generation\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            if past_key_values:\n",
        "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"position_ids\": position_ids,\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
        "            )\n",
        "        return reordered_past\n",
        "\n",
        "class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel):\n",
        "    _tied_weights_keys = [\"embed_out.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.gpt_neox = GPTNeoXModel(config)\n",
        "        self.embed_out = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.embed_out\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.embed_out = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        true_false_labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.gpt_neox(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        lm_logits = self.embed_out(hidden_states)\n",
        "\n",
        "        lm_loss = torch.tensor(0.).to(lm_logits.device)\n",
        "\n",
        "        if labels is not None and true_false_labels is not None:\n",
        "            loss_fct_1 = CrossEntropyLoss()\n",
        "            loss_fct_2 = NLLLoss()\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Enable model parallelism\n",
        "            true_indices = torch.where(true_false_labels == 1)[0]\n",
        "            false_indices = torch.where(true_false_labels == 0)[0]\n",
        "            if true_indices.shape[0] != 0:\n",
        "                true_logits = torch.index_select(shift_logits, dim=0, index=true_indices)\n",
        "                true_labels = torch.index_select(shift_labels, dim=0, index=true_indices)\n",
        "                true_logits = true_logits.view(-1, self.config.vocab_size)\n",
        "                true_labels = true_labels.view(-1)\n",
        "                true_labels = true_labels.to(true_logits.device)\n",
        "                lm_loss = lm_loss + loss_fct_1(true_logits, true_labels)\n",
        "            if false_indices.shape[0] != 0:\n",
        "                false_logits = torch.index_select(shift_logits, dim=0, index=false_indices)\n",
        "                false_labels = torch.index_select(shift_labels, dim=0, index=false_indices)\n",
        "                false_logits = false_logits.view(-1, self.config.vocab_size)\n",
        "                false_labels = false_labels.view(-1)\n",
        "                false_labels = false_labels.to(false_logits.device)\n",
        "                lm_loss = lm_loss - 0.05 * loss_fct_2(torch.softmax(false_logits,dim=-1), false_labels)\n",
        "        # t_y_f_y\n",
        "        if labels is not None and true_false_labels is None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            lm_loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        input_shape = input_ids.shape\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past_key_values is not None:\n",
        "            past_length = past_key_values[0][0].shape[2]\n",
        "\n",
        "            # Some generation methods already pass only the last input ID\n",
        "            if input_ids.shape[1] > past_length:\n",
        "                remove_prefix_length = past_length\n",
        "            else:\n",
        "                # Default to old behavior: keep only final ID\n",
        "                remove_prefix_length = input_ids.shape[1] - 1\n",
        "\n",
        "            input_ids = input_ids[:, remove_prefix_length:]\n",
        "\n",
        "        position_ids = kwargs.get(\"position_ids\", None)\n",
        "        if attention_mask is not None and position_ids is None:\n",
        "            # create position_ids on the fly for batch generation\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            if past_key_values:\n",
        "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
        "\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"position_ids\": position_ids,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def _reorder_cache(self, past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n",
        "                + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "class OPTForCausalLM(OPTPreTrainedModel):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = OPTModel(config)\n",
        "        # the lm_head weight is automatically tied to the embed tokens weight\n",
        "        self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.decoder.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model.decoder = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.decoder\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        true_false_labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model.decoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        logits = self.lm_head(outputs[0]).contiguous()\n",
        "\n",
        "        loss = torch.tensor(0.).to(logits.device)\n",
        "\n",
        "        #t_y_f_n or con\n",
        "        if labels is not None and true_false_labels is not None:\n",
        "            loss_fct_1 = CrossEntropyLoss()\n",
        "            loss_fct_2 = NLLLoss()\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Enable model parallelism\n",
        "            true_indices = torch.where(true_false_labels == 1)[0]\n",
        "            false_indices = torch.where(true_false_labels == 0)[0]\n",
        "            if true_indices.shape[0] != 0:\n",
        "                true_logits = torch.index_select(shift_logits, dim=0, index=true_indices)\n",
        "                true_labels = torch.index_select(shift_labels, dim=0, index=true_indices)\n",
        "                true_logits = true_logits.view(-1, self.config.vocab_size)\n",
        "                true_labels = true_labels.view(-1)\n",
        "                true_labels = true_labels.to(true_logits.device)\n",
        "                loss = loss + loss_fct_1(true_logits, true_labels)\n",
        "            if false_indices.shape[0] != 0:\n",
        "                false_logits = torch.index_select(shift_logits, dim=0, index=false_indices)\n",
        "                false_labels = torch.index_select(shift_labels, dim=0, index=false_indices)\n",
        "                false_logits = false_logits.view(-1, self.config.vocab_size)\n",
        "                false_labels = false_labels.view(-1)\n",
        "                false_labels = false_labels.to(false_logits.device)\n",
        "                loss = loss - 0.05 * loss_fct_2(torch.softmax(false_logits,dim=-1), false_labels)\n",
        "            # Flatten the tokens\n",
        "        # t_y_f_y\n",
        "        if labels is not None and true_false_labels is None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return (loss,) + output if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "def main():\n",
        "    global modules_list\n",
        "    dataset = load_dataset(\"json\", data_files=\"/mnt/nas-coai-wlcb/fhwexp/chatgpt-out/XSUM-final.json\")\n",
        "    dataset = dataset[\"train\"].shuffle(seed=42).train_test_split(test_size=0.1)\n",
        "    # dataset = load_dataset(\"json\", data_files=\"/mnt/nas-coai-wlcb/fhwexp/chatgpt-out/XSUM-combine-test.json\")\n",
        "    # dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "    probing_dataset = load_dataset(\"json\", data_files=\"/mnt/nas-coai-wlcb/fhwexp/chatgpt-out/defact.json\")\n",
        "    # train_dataset = load_dataset(\"json\", data_files=\"/mnt/nas-coai-wlcb/fhwexp/chatgpt-out/XSUM-combine-train.json\")\n",
        "    # test_dataset = load_dataset(\"json\", data_files=\"/mnt/nas-coai-wlcb/fhwexp/chatgpt-out/XSUM-combine-test.json\")\n",
        "    if args.mode == 't_y_f_y':\n",
        "        train_dataset = dataset[\"train\"].map(batch_preprocessing_t_y_f_y, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        test_dataset = dataset[\"test\"].map(batch_preprocessing_t_y_f_y, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        probing_dataset = probing_dataset[\"train\"].map(batch_preprocessing_t_y_f_n, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "    if args.mode == 't_y_f_n':\n",
        "        train_dataset = dataset[\"train\"].map(batch_preprocessing_t_y_f_n, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        test_dataset = dataset[\"test\"].map(batch_preprocessing_t_y_f_n, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "    if args.mode == 'con' or args.mode == 'lora':\n",
        "        train_dataset = dataset[\"train\"].map(batch_preprocessing_con, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        test_dataset = dataset[\"test\"].map(batch_preprocessing_con, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        probing_dataset = probing_dataset[\"train\"].map(batch_preprocessing_t_y_f_n, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "    if args.mode == 'procon' or args.mode == 'proun':\n",
        "        train_dataset = dataset[\"train\"].map(batch_preprocessing_con, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        test_dataset = dataset[\"test\"].map(batch_preprocessing_con, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "        probing_dataset = probing_dataset[\"train\"].map(batch_preprocessing_t_y_f_n, batched=True, batch_size=args.per_device_train_batch_size, remove_columns=[\"article\",\"summary\",\"label\"], num_proc=1)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='ckpt/pythia-'+args.mode+f'/{args.cur_epoch}',\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "        fp16=True,  # T5 overflows with fp16\n",
        "        bf16=False,  # Use BF16 if available\n",
        "        learning_rate=args.lr,\n",
        "        warmup_ratio=0.2,\n",
        "        weight_decay=3e-7,\n",
        "        num_train_epochs=1,\n",
        "        deepspeed=args.deepspeed,\n",
        "        # logging & evaluation strategies\n",
        "        logging_dir=\"logs/pythia-\"+args.mode,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        label_names = label_names,\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "    if args.cur_epoch == 0:\n",
        "        model = GPTNeoXForCausalLM.from_pretrained(\"../model/\"+args.model_name, torch_dtype=torch.float16)\n",
        "    else:\n",
        "        fs = open('/mnt/nas-coai-wlcb/fhwexp/prefix-tuning/pythia-modules.json', 'r')\n",
        "        lines = fs.readlines()\n",
        "        modules_list = json.loads(lines[0])[\"modules\"]\n",
        "        model = GPTNeoXForCausalLM.from_pretrained('/mnt/nas-coai-wlcb/fhwexp/prefix-tuning/ckpt/pythia-'+args.mode+f'/{args.cur_epoch-1}/checkpoint-1841', torch_dtype=torch.float16)\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "        if name in modules_list:\n",
        "            param.requires_grad = True\n",
        "    data_collator = DefaultDataCollator()\n",
        "    trainer = MyTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset={\"probing\": probing_dataset, \"test\": test_dataset},\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    trainer.train()\n",
        "    # model = AutoModelForCausalLM.from_pretrained(\"../model/\"+args.model_name, torch_dtype=torch.float16)\n",
        "    # model = LlamaForCausalLM.from_pretrained(\"../model/\"+args.model_name, torch_dtype=torch.float16)\n",
        "\n",
        "    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vtllbuHrnXl5",
        "outputId": "012e2855-ad15-4b1a-fa84-a9449ca7c0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-47e19c78961b>\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.opt.modeling_opt because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ntype object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDevicesHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torchao_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_model_to_fp8_ao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_first_and_last_linear_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_ao_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m from .constants import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/ao.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torchao_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_linear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloat8LinearConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from torchao.quantization import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mautoquant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .autoquant import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mALL_AUTOQUANT_CLASS_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/autoquant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torchao.dtypes import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mAffineQuantizedTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maffine_quantized_tensor_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .affine_quantized_tensor import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mAffineQuantizedTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/affine_quantized_tensor_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m from torchao.dtypes.floatx.cutlass_semi_sparse_layout import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0m_linear_fp8_act_fp8_weight_sparse_cutlass_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/floatx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .cutlass_semi_sparse_layout import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCutlassSemiSparseLayout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/floatx/cutlass_semi_sparse_layout.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAQTTensorImpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from torchao.ops import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mrowwise_scaled_linear_sparse_cutlass_f8f8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m\"mx_fp8_bf16(Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -> Tensor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_fixed_stride_order\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from ...utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1463\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1475\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ntype object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-47e19c78961b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1463\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1475\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.opt.modeling_opt because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ntype object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'"
          ]
        }
      ]
    }
  ]
}